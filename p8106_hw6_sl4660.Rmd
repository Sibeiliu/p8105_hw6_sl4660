---
title: "hw6_sl4660"
author: "Sibei Liu"
date: "2019/11/24"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo= TRUE,
                      message=FALSE,warning=FALSE
                      )
```
# Problem 1


```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```

```{r}
child_weight=read_csv("./data/birthweight.csv") %>% 
  drop_na() %>% 
  mutate(
 babysex=factor(babysex,levels=c(1,2),labels=c("male","female")),
 frace=factor(frace,levels=c(1,2,3,4,8,9),labels=c( "White", "Black", "Asian", "Puerto Rican","Other", "Unknown")),
 mrace=factor(mrace,levels=c(1,2,3,4,8),labels=c( "White", "Black", "Asian", "Puerto Rican","Other")),
 malform=factor(malform,levels=c(0,1),labels=c("absent","present"))
)
```

## Let's build a model
Here is my model building process
First add all variables into the model(like backward method)
```{r}
rm1=lm(bwt~.,data=child_weight)
summary(rm1)
```
then remove the variable with p_value smaller than 0.05, then the babysex, bhead, blength,delwt,gaweeks,mrace,parity,smoken retains. Next, re-fit the model with above fitered variables.

```{r}
rm1_1=lm(bwt~babysex+bhead+blength+delwt+gaweeks+mrace+parity+smoken,data=child_weight)
summary(rm1_1)
```
We find all variables in this model is significant.
That is what I will use.

## The plot of model residual

```{r}
add_child_weight=child_weight %>% 
  add_predictions(rm1_1) %>% 
  add_residuals (rm1_1) 

  ggplot(add_child_weight,aes(x = pred, y = resid)) + 
    geom_point(alpha = 0.3) +
  labs(
        title = " Residuals vs. Predicted Values",
        x = "Predicted Birthweight (g)",
        y = "Residuals"
      )+
 geom_hline(yintercept = 0,col = "red",linetype = "dashed")
```

The residual is bouncing around the y=0 line, meaning with conatant variance, which indicates the model is reaonable.

## Compare my model to two others:

```{r}
cv_df = 
  crossv_mc(child_weight, 100)
```

```{r}
cv_df2 =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df3 = 
  cv_df2 %>% 
  mutate(rm1_mod  = map(train, ~lm(bwt~babysex+bhead+blength+delwt+gaweeks+mrace+parity+smoken, data = .x)),
         rm2_mod  = map(train, ~lm(bwt~blength+gaweeks, data = .x)),
         rm3_mod  = map(train, ~lm(bwt~bhead*blength*babysex, data = .x))) %>% 
  mutate(rmse_rm1 = map2_dbl(rm1_mod , test, ~rmse(model = .x, data = .y)),
         rmse_rm2 = map2_dbl(rm2_mod , test, ~rmse(model = .x, data = .y)),
         rmse_rm3 = map2_dbl(rm3_mod, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df4=cv_df3 %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") 

  cv_df5=mutate(cv_df4,
         model = fct_reorder(model,rmse))
    ggplot(cv_df5,aes(x = model, y = rmse)) + geom_violin()
    
  cv_df5 %>% 
    group_by(model) %>% 
    summarise(mean_rmse=mean(rmse))
```

The smaller rmse is, the better the model is. So the first model I created using babysex, bhead,blength,delwt,gaweeks,mrace, parity,smoken is better. We also can read the mean of rmse in each model above. 

# Problem 2
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
boot_sample = function(df) {
  sample_frac(df,size=1, replace = TRUE)
}
```


```{r}
boot_straps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )
```

```{r}
bootsrap_results=boot_straps %>% 
  mutate(
    models = map(strap_sample, ~lm(tmax~tmin,data=.x)),
    results=map(models,broom::glance)
    )%>% 
  select(-strap_sample,-models) %>% 
  unnest(results) %>% 
  select(adj.r.squared) 
```


